---
categories:
- ""
- ""
date: "2017-10-31T21:28:43-05:00"
description: ""
draft: false
image: Coding.jpg
keywords: ""
slug: first
title: My First Codes
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(rvest) # to scrape wikipedia page
```

# Analysis of Movies- IMDB dataset

We will look at a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset).

```{r,load_movies, warning=FALSE, message=FALSE}

movies <- read_csv(here::here("data", "movies.csv"))
glimpse(movies)

```

## Use your data import, inspection, and cleaning skills to answer the following:

-   Are there any missing values (NAs)? Are all entries distinct or are there duplicate entries?

While it does not appear that there were any missing values in the dataset, it does appear as though there were duplicate entries. This is because in the original dataset above,there were 2,961 rows of data. After removing potential duplicates by the distinct identifier "Title", there are now only 2,907 rows of data, suggesting that duplicate movie titles had been within the data. When further observing the raw data, the movie titles that were duplicated only differed in the "votes" category. This may be because the data was updated at a later date, and the original entry was not removed.
```{r data_cleaning}
#Search for missing values (NAs)
sum(is.na(movies)) #Does not appear there are any missing values

#Search for duplicate entries based on title
sum(duplicated(movies$title))

#Remove duplicate rows based on Title (distinct identifier)
movies %>% 
  distinct(title, .keep_all = TRUE)

```


```{r, table_moviegenre}
#Create table of movie genres
movies %>%
    group_by(genre) %>% #group by genre
    count(sort=TRUE) #rank by descending order

```

```{r, table_moviereturns}
#Create table with average gross earnings and budget
movies %>%
  group_by(genre) %>% #sort by genre
  summarize(average_gross = mean(gross), 
         average_budget = mean(budget),
         return_on_budget = (average_gross/average_budget) -1) %>% #create avg. variables, and return on budget variable
  arrange(desc(return_on_budget)) #rank in descending order

```

```{r, directors_highestgross}
#Create table showing top 15 directors by gross revenue
movies %>%
  group_by(director) %>% #sort by director
  summarize(average_gross = mean(gross), 
         median_gross = median(gross),
         std_dev_gross = sd(gross),
          total_gross = sum(gross)) %>% #create mean, median, standard deviation, and total gross revenue variables
  arrange(desc(total_gross)) %>% #arrange in descending order by total gross revenue
  head(directors_highestgross, n=15) #only include the top 15 directors based on total gross revenue
 


```

```{r, ratings_bygenre}
#Create table describing ratings by genre
movies %>%
  group_by(genre)%>% #sort by genre
  summarise(mean_rating = mean(rating), 
            min_rating = min(rating), 
            max_rating=max(rating), 
            median_rating = median(rating), 
            sd_rating = sd(rating)) %>%  #create summary statistics on ratings
  arrange(desc(mean_rating)) #arranged in descending order based on mean rating

#Create histogram
ggplot(movies, aes(x=rating, fill = genre)) +
  geom_histogram() +
  labs(title = "Ratings Distributed by Genre",
       x="Ratings",
       y="Count")

```


## Use `ggplot` to answer the following

-   Examine the relationship between `gross` and `cast_facebook_likes`. Produce a scatterplot and write one sentence discussing whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office. What variable are you going to map to the Y- and X- axes?

The number of facebook likes that a cast receives is not likely to be a good predictor of how much money a movie will make at the box office, as seen through the lack of correlation in the graph and the correlation test. Since we were testing if gross revenue is *dependent* on the number of facebook likes, gross revenue became the dependent variable (y-axis) and facebook likes became the independent variable (x-axis).

```{r, gross_on_fblikes}
#Create scatterplot
ggplot(movies, aes(x=cast_facebook_likes, y=gross)) + #assign x- and y-axis variables 
  geom_point() +
  geom_smooth(method="lm") +
  scale_y_log10()+
  scale_x_log10() + #scale axis to make graph readable
  labs(title = "Gross Revenue of Movie vs Cast Facebook Likes",
       x="Cast Facebook Likes",
       y="Gross Revenue") 

#Test correlation of variables
cor(x=movies$cast_facebook_likes, y=movies$gross)

```

-   Examine the relationship between `gross` and `budget`. Produce a scatterplot and write one sentence discussing whether budget is likely to be a good predictor of how much money a movie will make at the box office.

Through the scatterplot and correlation test we can assume that the budget can be a good predictor of how much money the movie will make. This makes sense since movies with bigger budgets tend to have more popular casts, better quality production, and more marketing campaigns.


```{r, gross_on_budget}
#Create scatterplot
ggplot(movies, aes(x=budget, y=gross)) + #assign x- and y-axis variables
  geom_point() +
  geom_smooth(method="lm") +
  scale_y_log10()+
  scale_x_log10() + #scale axis to make graph readable
  labs(title = "Gross Revenue of Movie vs Movie Budget",
       x="Movie Budget",
       y="Gross Revenue") 

#Test correlation of variables
cor(x=movies$budget, y=movies$gross)

```

-   Examine the relationship between `gross` and `rating`. Produce a scatterplot, faceted by `genre` and discuss whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. Is there anything strange in this dataset?

The relationship between IMDB Movie Rating and the amount of money a movie will make at the box office varies depending on genre, as seen in the scatterplots below. For some genres, like fantasy, horror, documentaries, and crime, there is seemingly no correlation of higher ratings receiving higher gross revenues. However, in some genres, like action and adventure, there is a slightly positive correlation with higher rated movies tending to attain higher amounts of success at the box office. For some genres (i.e., family and musical), there may be a positive correlation between the rating and gross profit, but there are not enough data points to come to a conclusive answer.

```{r, gross_on_rating}
#Create graph
ggplot(movies, aes(x=rating, y=gross)) + #Assign x- and y-axis variables
  geom_point() +
  geom_smooth(method="lm") +
  labs(title = "Gross Revenue of Movie vs IMDB Rating",
       x="IMDB Movie Rating",
       y="Gross Revenue") +
  facet_wrap(~genre) #group scatterplots by genre

```

# Returns of financial stocks

```{r load_nyse_data, message=FALSE, warning=FALSE}
nyse <- read_csv(here::here("data","nyse.csv"))
```

```{r companies_per_sector}
#Create table on companies per sector
stocks <- nyse %>%
  group_by(sector) %>% #Group data by sector
  summarize(count = n()) #Sort data in descending order

#Create bar plot
ggplot(stocks, aes(x=count, y=reorder(sector, count), fill=sector)) +
  geom_bar(stat="identity") +
  theme(legend.position="none") +
  labs(title="Company Count by Sector", 
       x="Number of Companies", y="Sector")


```

```{r, tickers_from_wikipedia}

djia_url <- "https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average"


#get tables that exist on URL
tables <- djia_url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called djia. 
# Use purr::map() to create a list of all tables in URL
djia <- map(tables, . %>% 
               html_table(fill=TRUE)%>% 
               clean_names())


# constituents
table1 <- djia[[2]] %>% # the second table on the page contains the ticker symbols
  mutate(date_added = ymd(date_added),
         
         # if a stock is listed on NYSE, its symbol is, e.g., NYSE: MMM
         # We will get prices from yahoo finance which requires just the ticker
         
         # if symbol contains "NYSE*", the * being a wildcard
         # then we jsut drop the first 6 characters in that string
         ticker = ifelse(str_detect(symbol, "NYSE*"),
                          str_sub(symbol,7,11),
                          symbol)
         )

# we need a vector of strings with just the 30 tickers + SPY
tickers <- table1 %>% 
  select(ticker) %>% 
  pull() %>% # pull() gets them as a sting of characters
  c("SPY") # and lets us add SPY, the SP500 ETF

```

Now let us download prices for all 30 DJIA constituents and the SPY ETF that tracks SP500 since January 1, 2020

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}
# Notice the cache=TRUE argument in the chunk options. Because getting data is time consuming, # cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks <- tickers %>% 
  tq_get(get  = "stock.prices",
         from = "2000-01-01",
         to   = Sys.Date()) %>% # Sys.Date() returns today's price
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame
```

Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.

```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

```{r summarise_monthly_returns}
#Create table summarizing each stock and 'SPY'
new_table = myStocks_returns_monthly %>%
  summarise(returns_mean = mean(monthly_returns),
            returns_median = median(monthly_returns),
            returns_min = min(monthly_returns),
             returns_max = max(monthly_returns),
            returns_SD = sd(monthly_returns)) #Create columns to display summary statistics of stocks and SPY
print(new_table)

 

```

```{r density_monthly_returns}
#Create density plot for each stock
myStocks_returns_monthly %>%
  ggplot(aes(x=monthly_returns))+ facet_wrap(vars(symbol)) + 
  labs(x= "Monthly Returns", y="Frequency of Return") +
  geom_density()

```

What can you infer from this plot? Which stock is the riskiest? The least risky?

From the various density plots, it is discernible that those stocks with a wider density plot are more risky. Owing to which, in our opinion, Apple and CRM were the riskiest stocks in the observed time frame. One the other hand, SPY seems to be the least risky. This is likely because SPY is an ETF and therefore aggregates risk, unlike individual stocks.


```{r risk_return_plot}
#Create plot showing expected monthly returns and risk of the stocks
new_table %>%
  ggplot(aes(x= returns_SD, y= returns_mean))+ 
  geom_point() + 
  ggrepel::geom_text_repel(aes(label = symbol)) + #Label stocks with tickers
  labs(
    title = "Relationship between SD and Expected Monthly Return",
    x = 'Standard Deviation of Monthly Returns',
    y = 'Expected monthly Return'
  ) +
  NULL


```

What can you infer from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?

It can be inferred that stocks with a higher risk also have a higher expected average monthly return. This follows standard economic theory - the higher the risk, the higher the return. Two stocks that stood out a bit for being relatively risky but without a corresponding higher expected return are DOW and CSCO. Both of the these names had relatively higher standard deviations, but they appeared to fall below the typical expected returns when compared to stocks with similar risk profiles. 

